{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# load StatHead player ids\n",
    "stathead = pd.read_csv('scraping/stat-head_player_data.csv')\n",
    "template_url = 'https://www.pro-football-reference.com/players/'\n",
    "\n",
    "# split the player names\n",
    "stathead[['first_name', 'last_name']] = stathead['Player'].str.split(' ', n=1, expand=True)\n",
    "\n",
    "# save the player page urls\n",
    "player_urls = []\n",
    "for last_name, id in zip(stathead['last_name'], stathead['PlayerId']):\n",
    "    player_urls.append(template_url + last_name[0] + '/' + id + '.htm')\n",
    "\n",
    "# get the tables\n",
    "pd.Series(player_urls).to_csv('scraping/player_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only retrieve data on the top fantasy players from each season. Additionally, we will save on the number of read operations by limiting the data we are trying to retrieve for each player. For example, we won't try to retrieve receiving stats for quarterbacks or passing stats for running backs. \n",
    "\n",
    "Currently, we only have player IDs for players active in 2023. We need all player IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.pro-football-reference.com/players/'\n",
    "target_section_id = 'div_players'\n",
    "outer_element = 'p'\n",
    "inner_element = 'a'\n",
    "target_attribute = 'href'\n",
    "\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "letters = np.random.permutation(list(letters))\n",
    "player_info = pd.DataFrame(columns=['player_name', 'player_url'])\n",
    "\n",
    "# proxies = pd.read_csv('proxies.csv')\n",
    "# proxies = np.random.permutation(proxies['proxy'])\n",
    "# i = 1\n",
    "\n",
    "for letter in letters:\n",
    "    url = base_url + letter\n",
    "    print(url)\n",
    "    # print(f'Attempting to use {proxies[i]}')\n",
    "    page = requests.get(url)\n",
    "    # while page.status_code != 200 and i < len(proxies):\n",
    "    #     i += 1\n",
    "    #     page = requests.get(url, proxies={\"http\": proxies[i], \"https\": proxies[i]})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    target_section = soup.find(id=target_section_id)\n",
    "    outer_elements = target_section.find_all(outer_element)\n",
    "    inner_elements = [element.find(inner_element) for element in outer_elements]\n",
    "\n",
    "    # this doesn't work as desired; results in list of lists\n",
    "    player_url = [element.get(target_attribute) for element in inner_elements]\n",
    "    player_name = [element.get_text() for element in inner_elements]\n",
    "    player_info = pd.concat([pd.DataFrame([[player_name, player_url]], columns=player_info.columns), player_info], ignore_index=True)\n",
    "    wait = np.random.randint(10, 15)\n",
    "    time.sleep(wait)\n",
    "\n",
    "# convert list of lists into dataframe and save\n",
    "tmp = player_info.copy()\n",
    "names = pd.Series()\n",
    "urls = pd.Series()\n",
    "for name, url in zip(tmp['player_name'], tmp['player_url']):\n",
    "    names = pd.concat([names, pd.Series(name)], ignore_index=True)\n",
    "    urls = pd.concat([urls, pd.Series(url)], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({'player_name': names, 'player_url': urls}).to_csv('player_urls_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the player urls\n",
    "# get the pages\n",
    "# get the relevant tables\n",
    "player_urls = pd.read_csv('player_urls_all.csv')\n",
    "example = player_urls[player_urls['player_name'] == 'Josh Allen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From example it is clear that there are duplicates that we will need to handle. The first thing we can do is remove players that don't play the positions of interest (QB, RB, WR, TE) from player_urls_all and save to player_urls_filtered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the pages for the players\n",
    "import os\n",
    "os.chdir('/Users/ryan-saloma/Python Projects/fantasy_football/')\n",
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_all = pd.read_csv('scraping/player_urls_all.csv')\n",
    "player_urls = player_urls_all['player_url']\n",
    "\n",
    "# Load league data from 1970 to 2023\n",
    "# Filter urls for players who played in this time frame\n",
    "# Remove duplicates\n",
    "# Problem: more than one player can have the same name\n",
    "\n",
    "# Change the directory to the league data\n",
    "# Concatenate the data\n",
    "league_data = pd.DataFrame()\n",
    "for year in range(1970, 2024):\n",
    "    file = f'data/league/clean/cleaned_fantasy_{year}.csv'\n",
    "    data = pd.read_csv(file)\n",
    "    league_data = pd.concat([league_data, data], ignore_index=True)\n",
    "\n",
    "# Get the player names\n",
    "players = league_data.drop_duplicates(subset=['player', 'position'])[['player', 'position']]\n",
    "\n",
    "# Load the player urls\n",
    "player_urls = pd.read_csv('scraping/player_urls_all.csv')\n",
    "\n",
    "# Match player names with player urls\n",
    "# Save the matched player urls\n",
    "matched_urls = pd.DataFrame()\n",
    "for name, position in zip(players['player'], players['position']):\n",
    "    matched = player_urls[player_urls['player_name'] == name]\n",
    "    matched['position'] = position\n",
    "    matched_urls = pd.concat([matched_urls, matched], ignore_index=True)\n",
    "\n",
    "matched_urls.to_csv('scraping/player_urls_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to get page for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m wait \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "\n",
    "# make this work in batches to reduce the number of pages stored at any given time\n",
    "for url in player_urls_filtered:\n",
    "    player_page = requests.get(base_player_url + url)\n",
    "    if player_page.status_code == 200:\n",
    "        with open('scraping/player_pages/' + url.split('/')[-1], 'w') as f:\n",
    "            f.write(player_page.text)\n",
    "    else:\n",
    "        print(f'Failed to get page for {url}')\n",
    "    wait = np.random.randint(2, 3)\n",
    "    time.sleep(wait)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
