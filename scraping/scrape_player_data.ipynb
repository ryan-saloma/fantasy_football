{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load StatHead player ids\n",
    "stathead = pd.read_csv('scraping/stat-head_player_data.csv')\n",
    "template_url = 'https://www.pro-football-reference.com/players/'\n",
    "\n",
    "# split the player names\n",
    "stathead[['first_name', 'last_name']] = stathead['Player'].str.split(' ', n=1, expand=True)\n",
    "\n",
    "# save the player page urls\n",
    "player_urls = []\n",
    "for last_name, id in zip(stathead['last_name'], stathead['PlayerId']):\n",
    "    player_urls.append(template_url + last_name[0] + '/' + id + '.htm')\n",
    "\n",
    "# get the tables\n",
    "pd.Series(player_urls).to_csv('scraping/player_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only retrieve data on the top fantasy players from each season. Additionally, we will save on the number of read operations by limiting the data we are trying to retrieve for each player. For example, we won't try to retrieve receiving stats for quarterbacks or passing stats for running backs. \n",
    "\n",
    "Currently, we only have player IDs for players active in 2023. We need all player IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.pro-football-reference.com/players/'\n",
    "target_section_id = 'div_players'\n",
    "outer_element = 'p'\n",
    "inner_element = 'a'\n",
    "target_attribute = 'href'\n",
    "\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "letters = np.random.permutation(list(letters))\n",
    "player_info = pd.DataFrame(columns=['player_name', 'player_url'])\n",
    "\n",
    "# proxies = pd.read_csv('proxies.csv')\n",
    "# proxies = np.random.permutation(proxies['proxy'])\n",
    "# i = 1\n",
    "\n",
    "for letter in letters:\n",
    "    url = base_url + letter\n",
    "    print(url)\n",
    "    # print(f'Attempting to use {proxies[i]}')\n",
    "    page = requests.get(url)\n",
    "    # while page.status_code != 200 and i < len(proxies):\n",
    "    #     i += 1\n",
    "    #     page = requests.get(url, proxies={\"http\": proxies[i], \"https\": proxies[i]})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    target_section = soup.find(id=target_section_id)\n",
    "    outer_elements = target_section.find_all(outer_element)\n",
    "    inner_elements = [element.find(inner_element) for element in outer_elements]\n",
    "\n",
    "    # this doesn't work as desired; results in list of lists\n",
    "    player_url = [element.get(target_attribute) for element in inner_elements]\n",
    "    player_name = [element.get_text() for element in inner_elements]\n",
    "    player_info = pd.concat([pd.DataFrame([[player_name, player_url]], columns=player_info.columns), player_info], ignore_index=True)\n",
    "    wait = np.random.randint(10, 15)\n",
    "    time.sleep(wait)\n",
    "\n",
    "# convert list of lists into dataframe and save\n",
    "tmp = player_info.copy()\n",
    "names = pd.Series()\n",
    "urls = pd.Series()\n",
    "for name, url in zip(tmp['player_name'], tmp['player_url']):\n",
    "    names = pd.concat([names, pd.Series(name)], ignore_index=True)\n",
    "    urls = pd.concat([urls, pd.Series(url)], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({'player_name': names, 'player_url': urls}).to_csv('player_urls_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the player urls\n",
    "# get the pages\n",
    "# get the relevant tables\n",
    "player_urls = pd.read_csv('player_urls_all.csv')\n",
    "example = player_urls[player_urls['player_name'] == 'Josh Allen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From example it is clear that there are duplicates that we will need to handle. The first thing we can do is remove players that don't play the positions of interest (QB, RB, WR, TE) from player_urls_all and save to player_urls_filtered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the pages for the players\n",
    "import os\n",
    "os.chdir('/Users/ryan-saloma/Python Projects/fantasy_football/')\n",
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_all = pd.read_csv('scraping/player_urls_all.csv')\n",
    "player_urls = player_urls_all['player_url']\n",
    "\n",
    "# Load league data from 1970 to 2023\n",
    "# Filter urls for players who played in this time frame\n",
    "# Remove duplicates\n",
    "# Problem: more than one player can have the same name\n",
    "\n",
    "# Change the directory to the league data\n",
    "# Concatenate the data\n",
    "league_data = pd.DataFrame()\n",
    "for year in range(1970, 2024):\n",
    "    file = f'data/league/clean/cleaned_fantasy_{year}.csv'\n",
    "    data = pd.read_csv(file)\n",
    "    league_data = pd.concat([league_data, data], ignore_index=True)\n",
    "\n",
    "# Get the player names\n",
    "players = league_data.drop_duplicates(subset=['player', 'position'])[['player', 'position']]\n",
    "\n",
    "# Load the player urls\n",
    "player_urls = pd.read_csv('scraping/player_urls_all.csv')\n",
    "\n",
    "# Match player names with player urls\n",
    "# Save the matched player urls\n",
    "matched_urls = pd.DataFrame()\n",
    "for name, position in zip(players['player'], players['position']):\n",
    "    matched = player_urls[player_urls['player_name'] == name]\n",
    "    matched['position'] = position\n",
    "    matched_urls = pd.concat([matched_urls, matched], ignore_index=True)\n",
    "\n",
    "matched_urls.to_csv('scraping/player_urls_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make this work in batches to reduce the number of pages stored at any given time\n",
    "for i in range(0, 906):\n",
    "    url = player_urls_filtered[i]\n",
    "    player_page = requests.get(base_player_url + url)\n",
    "    if player_page.status_code == 200:\n",
    "        with open('scraping/player_pages/' + url.split('/')[-1], 'w') as f:\n",
    "            f.write(player_page.text)\n",
    "    else:\n",
    "        print(f'Failed to get page for {url}')\n",
    "    wait = np.random.randint(2, 3)\n",
    "    time.sleep(wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(page):\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # div id=\"info\" > div id=\"meta\" > div > 2nd p tag > text\n",
    "    position = soup.find('div', {'id': 'info'}).find('div', {'id': 'meta'}).find_all('p')[1].get_text()\n",
    "    return clean_position_text(position)\n",
    "\n",
    "def clean_position_text(position):\n",
    "    position = position.replace('Position:', '').strip()\n",
    "    # remove \"Throws:\" and \"Right\" or \"Left\"\n",
    "    if 'Throws:' in position:\n",
    "        position = position.split('Throws:')[0].strip()\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through files in player_pages\n",
    "base_path = 'scraping/player_pages/'\n",
    "files = [file for file in os.listdir(base_path) if file.endswith('.htm')]\n",
    "positions = pd.DataFrame(columns=['player_url', 'position'])\n",
    "\n",
    "for file in files:\n",
    "    with open(base_path + file, 'r') as f:\n",
    "        page = f.read()\n",
    "        position = get_position(page)\n",
    "        positions = pd.concat([positions, pd.DataFrame([[file, position]], columns=positions.columns)], ignore_index=True)\n",
    "\n",
    "positions.to_csv('scraping/player_urls_with_positions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_player_pages(start, end):\n",
    "    base_path = 'scraping/player_pages/'\n",
    "    base_player_url = 'https://www.pro-football-reference.com'\n",
    "    player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "    for i in range(start, end):\n",
    "        url = player_urls_filtered[i]\n",
    "        player_page = requests.get(base_player_url + url)\n",
    "        if player_page.status_code == 200:\n",
    "            with open('scraping/player_pages/' + url.split('/')[-1], 'w') as f:\n",
    "                f.write(player_page.text)\n",
    "        else:\n",
    "            print(f'Failed to get page for {url}')\n",
    "        wait = np.random.randint(2, 3)\n",
    "        time.sleep(wait)\n",
    "\n",
    "    # Loop through files in player_pages\n",
    "    base_path = 'scraping/player_pages/'\n",
    "    files = [file for file in os.listdir(base_path) if file.endswith('.htm')]\n",
    "    positions = pd.DataFrame(columns=['player_url', 'position'])\n",
    "\n",
    "    for file in files:\n",
    "        with open(base_path + file, 'r') as f:\n",
    "            page = f.read()\n",
    "            position = get_position(page)\n",
    "            positions = pd.concat([positions, pd.DataFrame([[file, position]], columns=positions.columns)], ignore_index=True)\n",
    "\n",
    "    positions.to_csv('scraping/player_urls_with_positions.csv', index=False)\n",
    "\n",
    "    # Load the player urls with positions\n",
    "    player_positions = pd.read_csv('scraping/player_urls_with_positions.csv')\n",
    "    player_positions_na = player_positions[player_positions['position'].isna()]\n",
    "    player_positions = player_positions.dropna()\n",
    "    player_positions = player_positions[~player_positions['position'].str.contains('QB|RB|WR|TE')]\n",
    "\n",
    "    # Concatenate player_positions with player_positions_na\n",
    "    player_positions = pd.concat([player_positions, player_positions_na], ignore_index=True)\n",
    "\n",
    "    # Get the files to delete\n",
    "    files = [file for file in os.listdir('scraping/player_pages/') if file.endswith('.htm')]\n",
    "    files_to_delete = [file for file in files if player_positions['player_url'].str.contains(file).any()]\n",
    "\n",
    "    for file in files_to_delete:\n",
    "        print(f'Deleting file: {file}')\n",
    "        os.remove('scraping/player_pages/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "if wd != '/Users/ryan-saloma/Python Projects/fantasy_football/':\n",
    "    os.chdir('/Users/ryan-saloma/Python Projects/fantasy_football/')\n",
    "player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "batch_process_player_pages(0, 7782)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 contains Date in Game Logs\n",
      "Table 2 contains AV in Passing, Regular Season\n",
      "Table 4 contains Passing_IAY in Advanced Passing, Air Yards\n",
      "Table 5 contains Passing_Bats in Advanced Passing, Accuracy\n",
      "Table 6 contains Passing_PktTime in Advanced Passing, Pressure\n",
      "Table 7 contains RPO_Plays in Advanced Passing, Play Type\n",
      "Table 8 contains Y/A+ in Adjusted Passing\n",
      "Table 9 contains Receiving_Y/R in Rushing and Receiving\n",
      "Table 10 contains Receiving_Y/R in Rushing and Receiving\n",
      "Table 11 contains Fumbles_Fmb in Defense and Fumbles\n",
      "Table 12 contains Fumbles_Fmb in Defense and Fumbles\n",
      "Table 13 contains Pts/G in Scoring Summary\n",
      "Table 14 contains Pts/G in Scoring Summary\n",
      "Table 15 contains Off._Pct in Snap Counts\n"
     ]
    }
   ],
   "source": [
    "# Get names of sections of interest\n",
    "sections = pd.read_csv('scraping/sections.csv')\n",
    "\n",
    "# Create a dict that maps a key word to a section header\n",
    "\n",
    "# Test getting tables from player pages\n",
    "base_path = 'scraping/player_pages/'\n",
    "\n",
    "# Get the tables for a QB\n",
    "player_page_name = 'AlleJo02.htm'\n",
    "tables_qb = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "# Repeat for a WR and RB \n",
    "player_page_name = 'SmitDe07.htm'\n",
    "tables_wr = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "player_page_name = 'McCaCh01.htm'\n",
    "tables_rb = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "# An alternative approach would be to use AI to classify the tables\n",
    "dict = {\n",
    "    'Date' : 'Game Logs',\n",
    "    'AV' : 'Passing, Regular Season',\n",
    "    'Passing_IAY' : 'Advanced Passing, Air Yards',\n",
    "    'Passing_Bats' : 'Advanced Passing, Accuracy',\n",
    "    'Passing_PktTime' : 'Advanced Passing, Pressure',\n",
    "    'RPO_Plays' : 'Advanced Passing, Play Type',\n",
    "    'Y/A+' : 'Adjusted Passing',\n",
    "    'FantPt' : 'Fantasy',\n",
    "    'Receiving_Ctch%' : 'Rushing and Receiving',\n",
    "    'Rushing_YBC/Att' : 'Advanced Rushing and Receiving',\n",
    "    'Fumbles_Fmb' : 'Defense and Fumbles',\n",
    "    'Pts/G' : 'Scoring Summary',\n",
    "    'Off._Pct' : 'Snap Counts'\n",
    "}\n",
    "\n",
    "# Function to flatten the MultiIndex\n",
    "def flatten_columns(df):\n",
    "    # Create a list of flattened column names\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        # Check if first level is unnamed\n",
    "        if col[0] == '' or 'Unnamed' in col[0]:\n",
    "            new_columns.append(col[1])  # Use the second level name\n",
    "        elif col[1] == '' or 'Unnamed' in col[1]:\n",
    "            next\n",
    "        else:\n",
    "            new_columns.append(f\"{col[0]}_{col[1]}\")  # Combine both levels\n",
    "    \n",
    "    df.columns = new_columns  # Set new columns\n",
    "    return df\n",
    "\n",
    "cols = []\n",
    "new_tables = []\n",
    "for table in tables_qb:\n",
    "    tmp = table\n",
    "    # check if the columns are multi-index\n",
    "    if isinstance(tmp.columns, pd.MultiIndex):\n",
    "        # flatten the columns\n",
    "        tmp = flatten_columns(tmp)\n",
    "    # concatenate the columns\n",
    "    # cols = cols + list(tmp.columns)\n",
    "    new_tables.append(tmp)\n",
    "\n",
    "i = 1\n",
    "for table in new_tables:\n",
    "    cols = table.columns\n",
    "    # # print table number and contents\n",
    "    for key in dict.keys():\n",
    "        if key in cols:\n",
    "            print(f'Table {i} contains {key} in {dict[key]}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Pos</th>\n",
       "      <th>No.</th>\n",
       "      <th>Games_G</th>\n",
       "      <th>Games_GS</th>\n",
       "      <th>Rushing_Att</th>\n",
       "      <th>Rushing_Yds</th>\n",
       "      <th>Rushing_TD</th>\n",
       "      <th>...</th>\n",
       "      <th>Receiving_Lng</th>\n",
       "      <th>Receiving_R/G</th>\n",
       "      <th>Receiving_Y/G</th>\n",
       "      <th>Receiving_Ctch%</th>\n",
       "      <th>Receiving_Y/Tgt</th>\n",
       "      <th>Touch</th>\n",
       "      <th>Y/Tch</th>\n",
       "      <th>YScm</th>\n",
       "      <th>RRTD</th>\n",
       "      <th>Fmb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>22</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>631</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>7.1</td>\n",
       "      <td>631</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>109</td>\n",
       "      <td>510</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109</td>\n",
       "      <td>4.7</td>\n",
       "      <td>510</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020*</td>\n",
       "      <td>24</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>102</td>\n",
       "      <td>421</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>12.0</td>\n",
       "      <td>103</td>\n",
       "      <td>4.2</td>\n",
       "      <td>433</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>25</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>122</td>\n",
       "      <td>763</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>6.3</td>\n",
       "      <td>763</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022*</td>\n",
       "      <td>26</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>124</td>\n",
       "      <td>762</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124</td>\n",
       "      <td>6.1</td>\n",
       "      <td>762</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>27</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>111</td>\n",
       "      <td>524</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "      <td>4.7</td>\n",
       "      <td>524</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>28</td>\n",
       "      <td>BUF</td>\n",
       "      <td>QB</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Career</td>\n",
       "      <td>Career</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>96</td>\n",
       "      <td>674</td>\n",
       "      <td>3696</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>6.0</td>\n",
       "      <td>675</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3708</td>\n",
       "      <td>56</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year     Age   Tm  Pos   No.  Games_G  Games_GS  Rushing_Att  \\\n",
       "0    2018      22  BUF   QB  17.0       12        11           89   \n",
       "1    2019      23  BUF   QB  17.0       16        16          109   \n",
       "2   2020*      24  BUF   QB  17.0       16        16          102   \n",
       "3    2021      25  BUF   QB  17.0       17        17          122   \n",
       "4   2022*      26  BUF   QB  17.0       16        16          124   \n",
       "5    2023      27  BUF   QB  17.0       17        17          111   \n",
       "6    2024      28  BUF   QB  17.0        3         3           17   \n",
       "7  Career  Career  NaN  NaN   NaN       97        96          674   \n",
       "\n",
       "   Rushing_Yds  Rushing_TD  ...  Receiving_Lng  Receiving_R/G  Receiving_Y/G  \\\n",
       "0          631           8  ...            0.0            0.0            0.0   \n",
       "1          510           9  ...            NaN            NaN            NaN   \n",
       "2          421           8  ...           12.0            0.1            0.8   \n",
       "3          763           6  ...            NaN            NaN            NaN   \n",
       "4          762           7  ...            NaN            NaN            NaN   \n",
       "5          524          15  ...            NaN            NaN            NaN   \n",
       "6           85           2  ...            NaN            NaN            NaN   \n",
       "7         3696          55  ...           12.0            0.0            0.1   \n",
       "\n",
       "   Receiving_Ctch%  Receiving_Y/Tgt  Touch  Y/Tch  YScm  RRTD  Fmb  \n",
       "0             0.0%              0.0     89    7.1   631     8    8  \n",
       "1              NaN              NaN    109    4.7   510     9   14  \n",
       "2           100.0%             12.0    103    4.2   433     9    9  \n",
       "3              NaN              NaN    122    6.3   763     6    8  \n",
       "4              NaN              NaN    124    6.1   762     7   13  \n",
       "5              NaN              NaN    111    4.7   524    15    7  \n",
       "6              NaN              NaN     17    5.0    85     2    2  \n",
       "7            50.0%              6.0    675    5.5  3708    56   61  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tables[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
