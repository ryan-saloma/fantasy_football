{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load StatHead player ids\n",
    "stathead = pd.read_csv('scraping/stat-head_player_data.csv')\n",
    "template_url = 'https://www.pro-football-reference.com/players/'\n",
    "\n",
    "# split the player names\n",
    "stathead[['first_name', 'last_name']] = stathead['Player'].str.split(' ', n=1, expand=True)\n",
    "\n",
    "# save the player page urls\n",
    "player_urls = []\n",
    "for last_name, id in zip(stathead['last_name'], stathead['PlayerId']):\n",
    "    player_urls.append(template_url + last_name[0] + '/' + id + '.htm')\n",
    "\n",
    "# get the tables\n",
    "pd.Series(player_urls).to_csv('scraping/player_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only retrieve data on the top fantasy players from each season. Additionally, we will save on the number of read operations by limiting the data we are trying to retrieve for each player. For example, we won't try to retrieve receiving stats for quarterbacks or passing stats for running backs. \n",
    "\n",
    "Currently, we only have player IDs for players active in 2023. We need all player IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.pro-football-reference.com/players/'\n",
    "target_section_id = 'div_players'\n",
    "outer_element = 'p'\n",
    "inner_element = 'a'\n",
    "target_attribute = 'href'\n",
    "\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "letters = np.random.permutation(list(letters))\n",
    "player_info = pd.DataFrame(columns=['player_name', 'player_url'])\n",
    "\n",
    "# proxies = pd.read_csv('proxies.csv')\n",
    "# proxies = np.random.permutation(proxies['proxy'])\n",
    "# i = 1\n",
    "\n",
    "for letter in letters:\n",
    "    url = base_url + letter\n",
    "    print(url)\n",
    "    # print(f'Attempting to use {proxies[i]}')\n",
    "    page = requests.get(url)\n",
    "    # while page.status_code != 200 and i < len(proxies):\n",
    "    #     i += 1\n",
    "    #     page = requests.get(url, proxies={\"http\": proxies[i], \"https\": proxies[i]})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    target_section = soup.find(id=target_section_id)\n",
    "    outer_elements = target_section.find_all(outer_element)\n",
    "    inner_elements = [element.find(inner_element) for element in outer_elements]\n",
    "\n",
    "    # this doesn't work as desired; results in list of lists\n",
    "    player_url = [element.get(target_attribute) for element in inner_elements]\n",
    "    player_name = [element.get_text() for element in inner_elements]\n",
    "    player_info = pd.concat([pd.DataFrame([[player_name, player_url]], columns=player_info.columns), player_info], ignore_index=True)\n",
    "    wait = np.random.randint(10, 15)\n",
    "    time.sleep(wait)\n",
    "\n",
    "# convert list of lists into dataframe and save\n",
    "tmp = player_info.copy()\n",
    "names = pd.Series()\n",
    "urls = pd.Series()\n",
    "for name, url in zip(tmp['player_name'], tmp['player_url']):\n",
    "    names = pd.concat([names, pd.Series(name)], ignore_index=True)\n",
    "    urls = pd.concat([urls, pd.Series(url)], ignore_index=True)\n",
    "\n",
    "pd.DataFrame({'player_name': names, 'player_url': urls}).to_csv('player_urls_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the player urls\n",
    "# get the pages\n",
    "# get the relevant tables\n",
    "player_urls = pd.read_csv('player_urls_all.csv')\n",
    "example = player_urls[player_urls['player_name'] == 'Josh Allen']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From example it is clear that there are duplicates that we will need to handle. The first thing we can do is remove players that don't play the positions of interest (QB, RB, WR, TE) from player_urls_all and save to player_urls_filtered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the pages for the players\n",
    "import os\n",
    "os.chdir('/Users/ryan-saloma/Python Projects/fantasy_football/')\n",
    "base_player_url = 'https://www.pro-football-reference.com'\n",
    "player_urls_all = pd.read_csv('scraping/player_urls_all.csv')\n",
    "player_urls = player_urls_all['player_url']\n",
    "\n",
    "# Load league data from 1970 to 2023\n",
    "# Filter urls for players who played in this time frame\n",
    "# Remove duplicates\n",
    "# Problem: more than one player can have the same name\n",
    "\n",
    "# Change the directory to the league data\n",
    "# Concatenate the data\n",
    "league_data = pd.DataFrame()\n",
    "for year in range(1970, 2024):\n",
    "    file = f'data/league/clean/cleaned_fantasy_{year}.csv'\n",
    "    data = pd.read_csv(file)\n",
    "    league_data = pd.concat([league_data, data], ignore_index=True)\n",
    "\n",
    "# Get the player names\n",
    "players = league_data.drop_duplicates(subset=['player', 'position'])[['player', 'position']]\n",
    "\n",
    "# Load the player urls\n",
    "player_urls = pd.read_csv('scraping/player_urls_all.csv')\n",
    "\n",
    "# Match player names with player urls\n",
    "# Save the matched player urls\n",
    "matched_urls = pd.DataFrame()\n",
    "for name, position in zip(players['player'], players['position']):\n",
    "    matched = player_urls[player_urls['player_name'] == name]\n",
    "    matched['position'] = position\n",
    "    matched_urls = pd.concat([matched_urls, matched], ignore_index=True)\n",
    "\n",
    "matched_urls.to_csv('scraping/player_urls_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make this work in batches to reduce the number of pages stored at any given time\n",
    "for i in range(0, 906):\n",
    "    url = player_urls_filtered[i]\n",
    "    player_page = requests.get(base_player_url + url)\n",
    "    if player_page.status_code == 200:\n",
    "        with open('scraping/player_pages/' + url.split('/')[-1], 'w') as f:\n",
    "            f.write(player_page.text)\n",
    "    else:\n",
    "        print(f'Failed to get page for {url}')\n",
    "    wait = np.random.randint(2, 3)\n",
    "    time.sleep(wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(page):\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # div id=\"info\" > div id=\"meta\" > div > 2nd p tag > text\n",
    "    position = soup.find('div', {'id': 'info'}).find('div', {'id': 'meta'}).find_all('p')[1].get_text()\n",
    "    return clean_position_text(position)\n",
    "\n",
    "def clean_position_text(position):\n",
    "    position = position.replace('Position:', '').strip()\n",
    "    # remove \"Throws:\" and \"Right\" or \"Left\"\n",
    "    if 'Throws:' in position:\n",
    "        position = position.split('Throws:')[0].strip()\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through files in player_pages\n",
    "base_path = 'scraping/player_pages/'\n",
    "files = [file for file in os.listdir(base_path) if file.endswith('.htm')]\n",
    "positions = pd.DataFrame(columns=['player_url', 'position'])\n",
    "\n",
    "for file in files:\n",
    "    with open(base_path + file, 'r') as f:\n",
    "        page = f.read()\n",
    "        position = get_position(page)\n",
    "        positions = pd.concat([positions, pd.DataFrame([[file, position]], columns=positions.columns)], ignore_index=True)\n",
    "\n",
    "positions.to_csv('scraping/player_urls_with_positions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_player_pages(start, end):\n",
    "    base_path = 'scraping/player_pages/'\n",
    "    base_player_url = 'https://www.pro-football-reference.com'\n",
    "    player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "    for i in range(start, end):\n",
    "        url = player_urls_filtered[i]\n",
    "        player_page = requests.get(base_player_url + url)\n",
    "        if player_page.status_code == 200:\n",
    "            with open('scraping/player_pages/' + url.split('/')[-1], 'w') as f:\n",
    "                f.write(player_page.text)\n",
    "        else:\n",
    "            print(f'Failed to get page for {url}')\n",
    "        wait = np.random.randint(2, 3)\n",
    "        time.sleep(wait)\n",
    "\n",
    "    # Loop through files in player_pages\n",
    "    base_path = 'scraping/player_pages/'\n",
    "    files = [file for file in os.listdir(base_path) if file.endswith('.htm')]\n",
    "    positions = pd.DataFrame(columns=['player_url', 'position'])\n",
    "\n",
    "    for file in files:\n",
    "        with open(base_path + file, 'r') as f:\n",
    "            page = f.read()\n",
    "            position = get_position(page)\n",
    "            positions = pd.concat([positions, pd.DataFrame([[file, position]], columns=positions.columns)], ignore_index=True)\n",
    "\n",
    "    positions.to_csv('scraping/player_urls_with_positions.csv', index=False)\n",
    "\n",
    "    # Load the player urls with positions\n",
    "    player_positions = pd.read_csv('scraping/player_urls_with_positions.csv')\n",
    "    player_positions_na = player_positions[player_positions['position'].isna()]\n",
    "    player_positions = player_positions.dropna()\n",
    "    player_positions = player_positions[~player_positions['position'].str.contains('QB|RB|WR|TE')]\n",
    "\n",
    "    # Concatenate player_positions with player_positions_na\n",
    "    player_positions = pd.concat([player_positions, player_positions_na], ignore_index=True)\n",
    "\n",
    "    # Get the files to delete\n",
    "    files = [file for file in os.listdir('scraping/player_pages/') if file.endswith('.htm')]\n",
    "    files_to_delete = [file for file in files if player_positions['player_url'].str.contains(file).any()]\n",
    "\n",
    "    for file in files_to_delete:\n",
    "        print(f'Deleting file: {file}')\n",
    "        os.remove('scraping/player_pages/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "if wd != '/Users/ryan-saloma/Python Projects/fantasy_football/':\n",
    "    os.chdir('/Users/ryan-saloma/Python Projects/fantasy_football/')\n",
    "player_urls_filtered = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "batch_process_player_pages(0, 7782)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get names of sections of interest\n",
    "# sections = pd.read_csv('scraping/sections.csv')\n",
    "\n",
    "# # Create a dict that maps a key word to a section header\n",
    "\n",
    "# # Test getting tables from player pages\n",
    "# base_path = 'scraping/player_pages/'\n",
    "\n",
    "# # Get the tables for a QB\n",
    "# player_page_name = 'AlleJo02.htm'\n",
    "# tables_qb = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "# # Repeat for a WR and RB \n",
    "# player_page_name = 'SmitDe07.htm'\n",
    "# tables_wr = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "# player_page_name = 'McCaCh01.htm'\n",
    "# tables_rb = pd.read_html(base_path + player_page_name)\n",
    "\n",
    "# # An alternative approach would be to use AI to classify the tables\n",
    "# dict = {\n",
    "#     'Date' : 'Game Logs',\n",
    "#     'AV' : 'Passing, Regular Season',\n",
    "#     'Passing_IAY' : 'Advanced Passing, Air Yards',\n",
    "#     'Passing_Bats' : 'Advanced Passing, Accuracy',\n",
    "#     'Passing_PktTime' : 'Advanced Passing, Pressure',\n",
    "#     'RPO_Plays' : 'Advanced Passing, Play Type',\n",
    "#     'Y/A+' : 'Adjusted Passing',\n",
    "#     'FantPt' : 'Fantasy',\n",
    "#     'Receiving_Succ%' : 'Rushing and Receiving',\n",
    "#     'Rushing_BrkTkl' : 'Advanced Rushing and Receiving',\n",
    "#     # 'Fumbles_Fmb' : 'Defense and Fumbles',\n",
    "#     # 'Pts/G' : 'Scoring Summary',\n",
    "#     'Off._Pct' : 'Snap Counts'\n",
    "# }\n",
    "\n",
    "# # Function to flatten the MultiIndex\n",
    "# def flatten_columns(df):\n",
    "#     # Create a list of flattened column names\n",
    "#     new_columns = []\n",
    "#     for col in df.columns:\n",
    "#         # Check if first level is unnamed\n",
    "#         if col[0] == '' or 'Unnamed' in col[0]:\n",
    "#             new_columns.append(col[1])  # Use the second level name\n",
    "#         elif col[1] == '' or 'Unnamed' in col[1]:\n",
    "#             next\n",
    "#         else:\n",
    "#             new_columns.append(f\"{col[0]}_{col[1]}\")  # Combine both levels\n",
    "    \n",
    "#     df.columns = new_columns  # Set new columns\n",
    "#     return df\n",
    "\n",
    "# cols = []\n",
    "# new_tables = []\n",
    "# for table in tables_qb:\n",
    "#     tmp = table\n",
    "#     # check if the columns are multi-index\n",
    "#     if isinstance(tmp.columns, pd.MultiIndex):\n",
    "#         # flatten the columns\n",
    "#         tmp = flatten_columns(tmp)\n",
    "#     # concatenate the columns\n",
    "#     # cols = cols + list(tmp.columns)\n",
    "#     new_tables.append(tmp)\n",
    "\n",
    "# i = 1\n",
    "# for table in new_tables:\n",
    "#     cols = table.columns\n",
    "#     # # print table number and contents\n",
    "#     for key in dict.keys():\n",
    "#         if key in cols:\n",
    "#             print(f'Table {i} contains {key} in {dict[key]}')\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the htm files in the player_pages directory\n",
    "# get the tables from each file\n",
    "# create a new folder for each player using their player id (characters before .htm)\n",
    "# save tables in the folder\n",
    "# label as 'table_{number}.csv'\n",
    "\n",
    "# Get the player ids\n",
    "player_urls = pd.read_csv('scraping/player_urls_filtered.csv')['player_url']\n",
    "player_ids = player_urls.str.split('/').str[-1].str.replace('.htm', '')\n",
    "\n",
    "# Get the files in the player_pages directory\n",
    "base_path = 'scraping/player_pages/'\n",
    "files = [file for file in os.listdir(base_path) if file.endswith('.htm')]\n",
    "\n",
    "# Get the tables for each player\n",
    "for file in files:\n",
    "    tables = pd.read_html(base_path + file)\n",
    "    player_id = file.replace('.htm', '')\n",
    "    player_path = f'scraping/player_tables/{player_id}/'\n",
    "    if not os.path.exists(player_path):\n",
    "        os.makedirs(player_path)\n",
    "    for i, table in enumerate(tables):\n",
    "        table.to_csv(player_path + f'table_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code extracts the player id from the player url\n",
    "\n",
    "# Derive column 'player_id' from column: 'player_url'\n",
    "def player_id(player_url):\n",
    "    \"\"\"\n",
    "    Transform based on the following examples:\n",
    "       player_url                   Output\n",
    "    1: \"/players/B/BaabMi20.htm\" => \"BaabMi20\"\n",
    "    \"\"\"\n",
    "    index1 = [i for i in range(len(player_url)) if player_url.startswith(\"/\", i)][2] + 1\n",
    "    return player_url[index1:player_url.find(\".\")]\n",
    "\n",
    "# Get dataframe\n",
    "df = pd.read_csv(\"scraping/player_urls_all.csv\")\n",
    "\n",
    "# Apply function\n",
    "df.insert(2, \"player_id\", df.apply(lambda row : player_id(row[\"player_url\"]), axis=1))\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"scraping/player_urls_all.csv\", index=False)\n",
    "\n",
    "# Repeat for player_urls_filtered.csv\n",
    "df = pd.read_csv(\"scraping/player_urls_filtered.csv\")\n",
    "df.insert(2, \"player_id\", df.apply(lambda row : player_id(row[\"player_url\"]), axis=1))\n",
    "df.to_csv(\"scraping/player_urls_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code reformats player_urls_all.csv to stathead_player_ids.csv\n",
    "# This csv will be used to create database\n",
    "\n",
    "# Load player_urls_all.csv\n",
    "df = pd.read_csv(\"scraping/player_urls_all.csv\")\n",
    "\n",
    "# Remove player_url column\n",
    "df = df.drop(columns=[\"player_url\"])\n",
    "\n",
    "# Make the player_id column the first column\n",
    "df = df[[\"player_id\", \"player_name\"]]\n",
    "\n",
    "# Save to csv\n",
    "df.to_csv(\"scraping/stathead_player_ids.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
