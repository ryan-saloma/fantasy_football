{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_table` finds a table with the id `fantasy`. Relying on the id only is not the most robust web-scraping approach. If the script cannot find a table with that id, it should try an alternative approach. This function is perhaps the most fragile.\n",
    "\n",
    "`get_headers` assumes that the table has two headers (similar to MultiIndex). \n",
    "\n",
    "`get_data` also rests on the assumption that the table includes two header rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the proxy\n",
    "# possible solution: use https://www.scraperapi.com/solutions/scraping-api/ to get the proxy\n",
    "def get_proxy():\n",
    "    try:\n",
    "        # get the proxy\n",
    "        proxy = requests.get('https://gimmeproxy.com/api/getProxy').json()\n",
    "        # get the ip and port\n",
    "        ip = proxy['ip']\n",
    "        port = proxy['port']\n",
    "        # create the proxy\n",
    "        proxy = {\n",
    "            'http': f'http://{ip}:{port}',\n",
    "            'https': f'https://{ip}:{port}'\n",
    "        }\n",
    "        return proxy\n",
    "    except Exception as e:\n",
    "        print('Error getting proxy')\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# get table from url with id fantasy\n",
    "# handle errors\n",
    "def get_table(url):\n",
    "    try:\n",
    "        # get and parsethe page\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # check if 429 error\n",
    "        # get proxy\n",
    "        if page.status_code == 429:\n",
    "            print('Error 429: Too many requests')\n",
    "            print('Getting proxy')\n",
    "            proxy = get_proxy()\n",
    "            print('Using proxy')\n",
    "            page = requests.get(url, proxies=proxy)\n",
    "\n",
    "        # find and return the table\n",
    "        # find the table with id fantasy\n",
    "        table = soup.find('table', {'id': 'fantasy'})\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        print('Error getting table')\n",
    "        # print the error\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# get the headers\n",
    "# get the first and second tr tags in the table\n",
    "def get_headers(table):\n",
    "    try:\n",
    "        # get the first two tr tags\n",
    "        trs = table.find_all('tr')\n",
    "        tr1 = trs[0]\n",
    "        tr2 = trs[1]\n",
    "\n",
    "        # get the first row of tags\n",
    "        th1 = tr1.find_all('th')\n",
    "        # get the second row of tags\n",
    "        th2 = tr2.find_all('th')\n",
    "\n",
    "        # get the text from the tags\n",
    "        th1 = [th.getText() for th in th1]\n",
    "        th2 = [th.getText() for th in th2]\n",
    "\n",
    "        # save the headers as two-row dataframe\n",
    "        headers = pd.DataFrame([th1, th2])\n",
    "        return headers\n",
    "    except Exception as e:\n",
    "        print('Error getting headers')\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# get the data\n",
    "def get_data(table):\n",
    "    try:\n",
    "        # get the rows\n",
    "        rows = table.find_all('tr')\n",
    "        # get the data from the rows\n",
    "        data = []\n",
    "        for row in rows[2:]:\n",
    "            # get the data from the row\n",
    "            td = row.find_all('td')\n",
    "            row = [i.getText() for i in td]\n",
    "            data.append(row)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print('Error getting data')\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# define the indexes\n",
    "# TODO: decide if we want to use MultiIndex\n",
    "# games = [6, 7]\n",
    "# passing = [8, 9, 10, 11, 12]\n",
    "# rushing = [13, 14, 15, 16]\n",
    "# receiving = [17, 18, 19, 20, 21]\n",
    "# fumbles = [22, 23]\n",
    "# scoring = [24, 25, 26]\n",
    "# fantasy_points = [27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "# change the names of the headers\n",
    "# 0 = rank, 1 = player, 2 = team, 3 = FantPostition, 4 = age, 5 = games, 6 = games started, 7 = passing completions\n",
    "# 8 = passing attempts, 9 = passing yards, 10 = passing touchdowns, 11 = interceptions, 12 = rushing attempts\n",
    "# 13 = rushing yards, 14 = rushing yards per attempt, 15 = rushing touchdowns, 16 = targets\n",
    "# 17 = receptions, 18 = receiving yards, 19 = receiving yards per reception, 20 = receiving touchdowns\n",
    "# 21 = fumbles, 22 = fumbles lost, 23 = scoring touchdowns, 24 = scoring 2-point conversions made\n",
    "# 25 = 2-point conversion passes, 26 = fantasy points, 27 = fantasy points ppr (point per reception) league\n",
    "# 28 = fantasy points draft kings, 29 = fantasy points fanduel, 30 = fantasy points above baseline, 31 = positional rank\n",
    "# 32 = overall rank\n",
    "\n",
    "# define the years: 1970-2023\n",
    "years = np.arange(1970, 2024)\n",
    "\n",
    "# loop through the years\n",
    "# wait between requests\n",
    "for year in years:\n",
    "    # define the url\n",
    "    url = f'https://www.pro-football-reference.com/years/{year}/fantasy.htm#fantasy'\n",
    "\n",
    "    # get the table, split into headers and data\n",
    "    table = get_table(url)\n",
    "    headers = get_headers(table)\n",
    "    data = get_data(table)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers.iloc[1, 1:])\n",
    "    \n",
    "    # save the data\n",
    "    df.to_csv(f'data/raw/league/fantasy_{year}.csv', index=False)\n",
    "\n",
    "    # wait between 1 and 5 seconds\n",
    "    wait = np.random.randint(1, 6)\n",
    "    time.sleep(wait)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
